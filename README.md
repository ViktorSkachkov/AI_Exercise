## How the programme works

Here are the actions you need to take to work with the programme.

First, you need to create a '.env' file, create a variable GEMINI_API_KEY and add a valid key.

You need to execute the command 'pip install -r requirements.txt' to get the necessary libraries.

You need to start the file 'ui-json_generation.py', upload the correct PDF file, extract the text and then get the JSON which would be generated by the AI model. You then save the JSON file locally.

After this, you need to start the file 'ui_enrich.py', upload the previously saved JSON file and then save the generated marketing summary and image prompt.

Finally, you need to start the file 'ui_image_generation.py', copy and paste the previously generated marketing summary and use it to generate an image. You then save the image locally.

## The output products

The output products which I already generated with this program (the JSON file, the short marketing summary, the image prompt and the image) are inside the folder 'AI_Output'

## Prompt design choices

- **Structured extraction (Step 2):** The prompt specifies an exact JSON schema (menu_items array, event object with title/date/description/location). We ask for "only valid JSON, no markdown or explanation" and use `response_mime_type=application/json` so the model returns parseable JSON. Rules explicitly forbid hallucination and ask for `null` or omission when data is missing. Ingredients are normalized to real ingredient names (e.g. "sweet banana" → "banana"); event description is kept concise and informative.
- **Enrichment (Step 3):** The prompt takes only the structured JSON (no raw PDF text) and requests two outputs in one call: `marketing_summary` (2–4 sentences, promotional) and `image_prompt` (one paragraph, visual and concrete). The image prompt that is returned would include things like the scene, style, mood, and key elements without including unnecessary details. Again we use JSON response mode and forbid inventing data. This keeps a single, clear separation: one call for extraction, one for creative enrichment.

## Assumptions made

- The PDF is a single page (or we use the first page’s content); multi-page PDFs are extracted in full but the task targets one-page content.
- Currency is normalized to EUR when the document suggests euros (e.g. "€"); other currencies are left as in the source or null.
- Image generation may be blocked by safety filters; the pipeline saves the image only when the API returns one, and reports when none is returned.

## Limitations and failure cases

- **LLM variability:** Output may vary between runs. Strict, descriptive prompts and JSON mode reduce but do not eliminate this.
- **Image generation:** Subject to provider safety and content policies; some prompts may return no image. Rate limits can also block text or image steps.
- **PDF quality:** The text cleaner handles common encoding and table issues well; unusual layouts may occasionally produce noisy input for the LLM.

## What you would improve with more time

- **API robustness:** Add retries with backoff for API calls to better handle rate limits and transient failures (image and text steps).
- **PDF handling:** Extend the text cleaner with more encoding and table patterns and optional language detection, so unusual layouts produce cleaner input for the LLM.
- **LLM consistency:** Validate JSON against a strict schema (e.g. Pydantic) and optionally re-prompt or fallback when the model returns inconsistent or malformed output.
- **Whole application:** Merging the different files, so they make one enterprise application with better UI and different pages for the different functions.